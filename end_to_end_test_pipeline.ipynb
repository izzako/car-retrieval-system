{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "from transformers import AutoModelForObjectDetection, AutoImageProcessor, pipeline\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "import torchvision.models as models\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "\n",
    "# import custom file\n",
    "\n",
    "from source_code.processor import vid_img_converter as v2img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_objects_from_image(image: Image.Image, labels: list, padding: int = 10):\n",
    "    \"\"\"\n",
    "    Crops objects from an image using bounding boxes with optional padding.\n",
    "    \n",
    "    Args:\n",
    "        image (PIL.Image): The source image.\n",
    "        labels (list): List of dicts with 'box' key containing 'xmin', 'ymin', 'xmax', 'ymax'.\n",
    "        padding (int): Number of pixels to pad around the bounding box.\n",
    "        \n",
    "    Returns:\n",
    "        List[PIL.Image]: Cropped image patches.\n",
    "    \"\"\"\n",
    "    width, height = image.size\n",
    "    cropped_images = []\n",
    "\n",
    "    for obj in labels:\n",
    "            box = obj['box']\n",
    "            xmin = max(box['xmin'] - padding, 0)\n",
    "            ymin = max(box['ymin'] - padding, 0)\n",
    "            xmax = min(box['xmax'] + padding, width)\n",
    "            ymax = min(box['ymax'] + padding, height)\n",
    "\n",
    "            cropped = image.crop((xmin, ymin, xmax, ymax))\n",
    "            cropped_images.append(cropped)\n",
    "\n",
    "    return cropped_images\n",
    "\n",
    "def load_model(checkpoint_path, num_classes, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    # LOAD THE CLASSIFIER MODEL\n",
    "    model = models.mobilenet_v3_large()\n",
    "    model.classifier[3] = nn.Linear(in_features=1280, out_features=num_classes)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "font = ImageFont.truetype(\"./ARIAL.TTF\",size=20)\n",
    "def plot_results(image, results, threshold=0.6):\n",
    "    image = Image.fromarray(np.uint8(image))\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    width, height = image.size\n",
    "\n",
    "    for result in results:\n",
    "        score = result[\"score\"]\n",
    "        label = result[\"label\"]\n",
    "        box = list(result[\"box\"].values())\n",
    "        if label in ['motorcycle','autorickshaw','rider']:\n",
    "            outline_color = 'red'\n",
    "        else:\n",
    "            outline_color = 'yellow'\n",
    "        if score > threshold:\n",
    "            x1, y1, x2, y2 = tuple(box)\n",
    "            draw.rectangle((x1, y1, x2, y2), outline=outline_color, width=3)\n",
    "            draw.text((x1 + 5, y1 - 20), label, fill=\"white\",font=font)\n",
    "            draw.text((x1 + 5, y1 + 10), f\"{score:.2f}\", fill=\"green\" if score > 0.7 else \"red\")\n",
    "\n",
    "    return image\n",
    "\n",
    "def infer_image(model, image: Image.Image, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    transform_x = v2.Compose([\n",
    "    v2.Resize((224, 224)),\n",
    "    v2.ToTensor(),\n",
    "    v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    class_names={\n",
    "        0: 'Coupe',\n",
    "        1: 'Sedan',\n",
    "        2: 'Cab',\n",
    "        3: 'Convertible',\n",
    "        4: 'SUV',\n",
    "        5: 'Minivan',\n",
    "        6: 'Hatchback',\n",
    "        7: 'Other',\n",
    "        8: 'Van',\n",
    "        9: 'Wagon'\n",
    "    }\n",
    "    input_tensor = transform_x(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        predicted_class = output.argmax(dim=1).item()\n",
    "\n",
    "    return class_names[predicted_class]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Video -> Image frames\n",
    "- set the `VIDEO_GDRIVE_ID` on the `.env` file\n",
    "-  download the video using `gdown` and save if to `data/test_video.mp4`\n",
    "- convert video to images in folder `\"data/frame_dir\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_id = os.environ.get(\"VIDEO_GDRIVE_ID\")\n",
    "vid_path = 'data/test_video.mp4'\n",
    "v2img.gdown.download(id=vid_id, output=vid_path, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid2img.video_to_images(vid_path,\"data/frame_dir/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Object Detection on Image Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"./data/frame_dir\"\n",
    "json_output_folder = \"./output/frame_json\"\n",
    "batch_size = 8  # adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# ðŸ§ª Create output folder\n",
    "os.makedirs(json_output_folder, exist_ok=True)\n",
    "\n",
    "# ðŸ“¦ Load pipeline\n",
    "\n",
    "# Load processor and model manually\n",
    "processor = AutoImageProcessor.from_pretrained(\"izzako/detr-resnet-50-finetuned-IDD_Detection\")\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    \"izzako/detr-resnet-50-finetuned-IDD_Detection\",\n",
    "    torch_dtype=\"auto\",          # Let HF decide best dtype\n",
    "    device_map=\"auto\"            # Avoid meta tensor issues\n",
    ")\n",
    "\n",
    "# Build the pipeline\n",
    "obj_detector = pipeline(\n",
    "    \"object-detection\",\n",
    "    model=model,\n",
    "    image_processor=processor,\n",
    "    # device=0  # or -1 for CPU\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/740 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 10/740 [00:04<06:06,  1.99it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 740/740 [05:46<00:00,  2.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“‚ Load image paths\n",
    "image_paths = sorted(list(Path(image_folder).glob(\"*.jpg\")))\n",
    "\n",
    "\n",
    "# ðŸ” Batch inference and save to JSON\n",
    "for i in tqdm(range(0, len(image_paths), batch_size),):\n",
    "    \n",
    "    batch_paths = image_paths[i:i+batch_size]\n",
    "    batch = [Image.open(p).convert(\"RGB\") for p in batch_paths]\n",
    "    results = obj_detector(batch)\n",
    "\n",
    "    for path, prediction in zip(batch_paths, results):\n",
    "        filename = Path(path).stem + \".json\"\n",
    "        output_path = Path(json_output_folder) / filename\n",
    "\n",
    "        # ðŸ“ Save prediction to JSON\n",
    "        with open(output_path, \"w\") as f:\n",
    "            json.dump(prediction, f, indent=2)\n",
    "\n",
    "        # print(f\"âœ… Saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Car Classifier on Detected Bounding Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"./data/frame_dir\"\n",
    "detection_folder = \"./output/frame_json\"\n",
    "batch_size = 8  # adjust as needed\n",
    "\n",
    "model = load_model(\"./output/mobilenet_v3_large_checkpoint_10.pth\", 10)\n",
    "\n",
    "labeled_folder =  \"./output/frame_label\"\n",
    "os.makedirs(labeled_folder,exist_ok=True)\n",
    "\n",
    "\n",
    "label_folder = \"./output/frame_label\"\n",
    "rec_plotted_img =  \"./output/pred_image\"\n",
    "\n",
    "smoothed_dir = './output/smoothed_frame_label'\n",
    "os.makedirs(smoothed_dir,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5920 [00:00<?, ?it/s]/projectnb/llamagrp/izzan/env/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5920/5920 [02:48<00:00, 35.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# RENAME DETECTED IMAGE TO CAR TYPE USING CLASSIFIER\n",
    "\n",
    "for j,label in enumerate(tqdm(sorted(os.listdir(detection_folder)))):\n",
    "    results = json.load(open(os.path.join(detection_folder,label),'r'))\n",
    "    image = Image.open(os.path.join(image_folder,sorted(os.listdir(image_folder))[j]))\n",
    "    cropped_imgs = crop_objects_from_image(image=image, labels=results, padding=20)\n",
    "    for i,result in enumerate(results):\n",
    "        if result['label']!='car':continue\n",
    "        pred_label = infer_image(model, cropped_imgs[i])\n",
    "        result['label']=pred_label\n",
    "\n",
    "    with open(os.path.join(labeled_folder,label), \"w\") as f:\n",
    "        json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## smoothing the bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(box1, box2):\n",
    "    \"\"\"Compute IoU between two boxes in [xmin, ymin, xmax, ymax] format.\"\"\"\n",
    "    xA = max(box1['xmin'], box2['xmin'])\n",
    "    yA = max(box1['ymin'], box2['ymin'])\n",
    "    xB = min(box1['xmax'], box2['xmax'])\n",
    "    yB = min(box1['ymax'], box2['ymax'])\n",
    "\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    box1Area = (box1['xmax'] - box1['xmin']) * (box1['ymax'] - box1['ymin'])\n",
    "    box2Area = (box2['xmax'] - box2['xmin']) * (box2['ymax'] - box2['ymin'])\n",
    "\n",
    "    return interArea / float(box1Area + box2Area - interArea + 1e-6)\n",
    "\n",
    "def smooth_boxes_across_frames(frames, iou_thresh=0.5, smoothing_weight=0.6):\n",
    "    \"\"\"\n",
    "    Match detections across frames using IoU and apply smoothing.\n",
    "    :param frames: List of detection results per frame.\n",
    "    :return: List of smoothed detection results.\n",
    "    \"\"\"\n",
    "    smoothed_frames = [frames[0]]  # first frame remains unchanged\n",
    "\n",
    "    for t in range(1, len(frames)):\n",
    "        prev_detections = smoothed_frames[-1]\n",
    "        curr_detections = frames[t]\n",
    "        matched = set()\n",
    "        new_detections = []\n",
    "\n",
    "        for curr in curr_detections:\n",
    "            best_iou = 0\n",
    "            best_prev = None\n",
    "\n",
    "            for prev in prev_detections:\n",
    "                # if prev['label'] != curr['label']:\n",
    "                #     continue\n",
    "                iou_score = iou(prev['box'], curr['box'])\n",
    "                if iou_score > best_iou:\n",
    "                    best_iou = iou_score\n",
    "                    best_prev = prev\n",
    "\n",
    "            if best_iou > iou_thresh and best_prev:\n",
    "                # Apply smoothing on matched boxes\n",
    "                smoothed_box = {}\n",
    "                for key in ['xmin', 'ymin', 'xmax', 'ymax']:\n",
    "                    smoothed_box[key] = (\n",
    "                        smoothing_weight * curr['box'][key] +\n",
    "                        (1 - smoothing_weight) * best_prev['box'][key]\n",
    "                    )\n",
    "                new_detections.append({'label': curr['label'], 'box': smoothed_box,'score':curr['score']})\n",
    "                matched.add(id(best_prev))\n",
    "            else:\n",
    "                # Unmatched, keep as-is\n",
    "                new_detections.append(curr)\n",
    "\n",
    "        smoothed_frames.append(new_detections)\n",
    "\n",
    "    return smoothed_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5920/5920 [00:16<00:00, 366.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# SMOOTHING\n",
    "results = []\n",
    "for i in tqdm(range(len(os.listdir(label_folder)))):\n",
    "    results.append(json.load(open(os.path.join(label_folder,sorted(os.listdir(label_folder))[i]),'r')))\n",
    "\n",
    "smoothed_results = smooth_boxes_across_frames(results, iou_thresh=0.8, smoothing_weight=0.6)\n",
    "for i,filename in enumerate(tqdm(sorted(os.listdir(label_folder)))):\n",
    "    json.dump(smoothed_results[i],open(os.path.join(smoothed_dir,filename),'w'),indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5920/5920 [02:53<00:00, 34.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# PLOT THE CLASSIFIER RESULT TO IMAGES WITH BOUNDING BOX\n",
    "os.makedirs(rec_plotted_img,exist_ok=True)\n",
    "for i in tqdm(range(len(os.listdir(smoothed_dir)))):\n",
    "    image_name=sorted(os.listdir(image_folder))[i]\n",
    "    image = Image.open(os.path.join(image_folder,image_name))\n",
    "    results = json.load(open(os.path.join(smoothed_dir,sorted(os.listdir(smoothed_dir))[i]),'r'))\n",
    "    plot_results(image, results, threshold=0.5).save(os.path.join(rec_plotted_img,image_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predicted Frames to Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating video of 197.00 seconds at 30.05 FPS.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 24/5920 [00:00<00:50, 117.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5920/5920 [00:53<00:00, 110.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to output/smooth_output_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "v2img.images_to_video('output/pred_image', 'output/smooth_output_video.mp4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
